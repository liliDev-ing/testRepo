{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "poetry_wasaj.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOFHASqDmXXq1jKsrjgfxTy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liliDev-ing/testRepo/blob/master/poetry_wasaj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create custom Tokenizer"
      ],
      "metadata": {
        "id": "y5zIwE1QdCkx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyJADDd4c0gZ"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "import datasets\n",
        "datasets.list_datasets()\n",
        "#use a sample of 1000 words from the arabic oscar dataset\n",
        "dataset = load_dataset(\n",
        "   'oscar', \n",
        "   'unshuffled_deduplicated_ar' ,\n",
        "    split='train[:1000]'\n",
        "   )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.mkdir('./source_poetry')\n",
        "os.mkdir('./image')\n",
        "os.mkdir('./topics')"
      ],
      "metadata": {
        "id": "6zq7BDbZdbfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download the sample dataset into local folder\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "text_data=[]\n",
        "file_count=0\n",
        "for sample in tqdm(dataset):\n",
        "  sample= sample['text'].replace('\\n', ' ')\n",
        "  text_data.append(sample)\n",
        "  #create .txt files from the oscar dataset\n",
        "  if len(text_data)== 1_000:\n",
        "    with open(f'./source_poetry_{file_count}.txt','w', encoding='utf-8')as fp:\n",
        "      fp.write('\\n'.join(text_data))\n",
        "    text_data=[]\n",
        "    file_count+=1"
      ],
      "metadata": {
        "id": "xJc-dJyydhCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "paths= [str(x) for x in Path('./source_poetry').glob('**/*.txt')]"
      ],
      "metadata": {
        "id": "caaXgEMMdy_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers\n",
        "from tokenizers import BertWordPieceTokenizer"
      ],
      "metadata": {
        "id": "aaEN3SN0d5_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create the Berttokenizer\n",
        "tokenizer= BertWordPieceTokenizer(\n",
        "    clean_text= True,\n",
        "    handle_chinese_chars= False,\n",
        "    strip_accents= False\n",
        ")"
      ],
      "metadata": {
        "id": "DlR1mJz1eRYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train the tokenizer and save it\n",
        "tokenizer.train( files= paths,\n",
        "                vocab_size= 10_000,\n",
        "                min_frequency= 2,\n",
        "                special_tokens= ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'],\n",
        "                limit_alphabet=1000,\n",
        "                wordpieces_prefix='##')\n",
        "os.mkdir('./new_tokenizer')\n",
        "tokenizer.save_model('./new_tokenizer')"
      ],
      "metadata": {
        "id": "VqC6ZM6OeYwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing the tokenizer\n",
        "!pip install transformers\n",
        "from transformers import BertTokenizer\n",
        "tokenizer= BertTokenizer.from_pretrained('./new_tokenizer')\n",
        "tokenizer('ما بال عينك منها الماء ينسكب.')"
      ],
      "metadata": {
        "id": "EmsoM2UjetRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# similarity between sentences"
      ],
      "metadata": {
        "id": "3Vic6aTVe_W7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#use the created tokenizer with pytorch to compute similarity between sentences\n",
        "#here is an example\n",
        "sentences =[\"أَراني إِذا هَوَّمتُ يا مَيُّ زُرتِني فَيا نِعمَتا لَو أَنَّ رُؤيايَ تَصدُقُ\",\n",
        "            \"لكل شيءٍ إذا ما تم نقصانُ فلا يُغرُّ بطيب العيش إنسانُ هي الأيامُ كما شاهدتها دُولٌ مَن سَرَّهُ زَمنٌ ساءَتهُ أزمانُ\",\n",
        "            \"وكنتُ أَرَى من وَجْهِ مَيّةَ لَمحةً فأَبْرَقُ مَغشيًّا علـيَّ مكانيـا وأَسمــعُ منــها نَبـأةً فكأنّـما أَصابَ بها\",\n",
        "            \"سَهمٌ طَريرٌ فؤاديا وَأَنصِبُ وَجهي نَحوَ مَكَّةَ بِالضُحى إِذا كانَ مِن فَرطِ اللَيالي بَدا ليا أُصلّي فما \",\n",
        "            \"مرحبا بالجميع في هذا المكان الجميل\",\n",
        "            \"أدري إذا ما ذكرتُها أثنتينِ صلّيتُ الضُّحى أم ثمانيا\"]"
      ],
      "metadata": {
        "id": "Zj48utYqfFqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel, BertModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "RvqdvnM7qAN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = AutoModel.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
        "\n",
        "# initialize dictionary that will contain tokenized sentences\n",
        "tokens = {'input_ids': [], 'attention_mask': []}\n",
        "\n",
        "for sentence in sentences:\n",
        "    # tokenize sentence and append to dictionary lists\n",
        "    new_tokens = tokenizer.encode_plus(sentence, max_length=128, truncation=True,\n",
        "                                       padding='max_length', return_tensors='pt')\n",
        "    tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
        "    tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
        "\n",
        "# reformat list of tensors into single tensor\n",
        "tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
        "tokens['attention_mask'] = torch.stack(tokens['attention_mask'])\n",
        "tokens['input_ids'].shape"
      ],
      "metadata": {
        "id": "-4HjzM4kfeC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**tokens)"
      ],
      "metadata": {
        "id": "HMyaD5xkfluZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = outputs.last_hidden_state\n",
        "attention_mask = tokens['attention_mask']\n",
        "mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
        "masked_embeddings = embeddings * mask\n",
        "summed = torch.sum(masked_embeddings, 1)\n",
        "summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
        "mean_pooled = summed / summed_mask"
      ],
      "metadata": {
        "id": "Ftged1WQfsmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "MM7qqgoRqs_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert from PyTorch tensor to numpy array\n",
        "mean_pooled = mean_pooled.detach().numpy()\n",
        "\n",
        "# calculate\n",
        "res = cosine_similarity(\n",
        "    [mean_pooled[0]],\n",
        "    mean_pooled[1:]\n",
        "    )"
      ],
      "metadata": {
        "id": "0MXYV7I8nXQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRPlUf3ZrGM6",
        "outputId": "453889e2-a691-4be4-f7a6-f93daabf4190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.83476675, 0.83850056, 0.8610432 , 0.83421755, 0.7608243 ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Link words with figures"
      ],
      "metadata": {
        "id": "1BICmU_pTPWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "6dS4c8Jgk-Pc"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "YVTh-RJzlDb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "import glob"
      ],
      "metadata": {
        "id": "J7uNs86ulAfj"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "st = ISRIStemmer()\n",
        "#word_list = \"ما بالُ عَينِكَ مِنها الماءُ يَنسَكِبُ كَأَنَّهُ مِن كُلى مَفرِيَّة سَرِبُ  \"\n",
        "word_list = input()\n",
        "\n",
        "def filter(word_list):\n",
        "    wordsfilter=[]\n",
        "    for a in word_tokenize(word_list):\n",
        "        stem = st.stem(a)\n",
        "        wordsfilter.append(stem)\n",
        "    return wordsfilter\n",
        "\n",
        "\n",
        "mm =filter(word_list)\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "images = [os.path.split(file)[1][:-4] for file in glob.glob('image/*.jpg')]\n",
        "gg = []\n",
        "for l in mm:\n",
        "    if l in images:\n",
        "        gg.append('image\\\\'+l+'.jpg')\n",
        "        \n",
        "%pylab inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "for i in gg:\n",
        "    img = mpimg.imread(i)\n",
        "    imgplot = plt.imshow(img)\n",
        "    plt.show()\n",
        "    print(os.path.split(i)[1][:-4])"
      ],
      "metadata": {
        "id": "wuBGXEcaT9pu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}